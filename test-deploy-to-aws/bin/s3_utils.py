import os
import sys
import pathlib
import boto3
import time

DEBUG = True

class MyException(Exception):
    pass

### --------------------------------------------------------------------------------------------
## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
### --------------------------------------------------------------------------------------------

cwd_contents = pathlib.Path.cwd().glob( "*" )   ### <------------------------- All FOLDERS + FILES in current folder

### NOTE: Keep following as RELATIVE_FILEPATHs ONLY.
### WARNING: do __NOT__ use ABSOLUTE_FILEPATHs.
LIST_of_LOCAL_PATHS = (
    # str( pathlib.Path( ".github" ) ),
    # str( pathlib.Path( "folder1" ) ),
    # str( pathlib.Path( "folder2" ) ),
    # str( pathlib.Path( "folder3/subfldr3" ) ),
    # str( pathlib.Path( "folder4/common.py" ) ),
    # str( pathlib.Path( "folder4/sf4/3deep/myscript.py" ) ),
    # str( pathlib.Path( "constants.py" ) ),
    str( pathlib.Path( "requirements.in" ) ),
    str( pathlib.Path( "requirements.txt" ) ),
    # str( pathlib.Path( "package.json" ) ),
    # str( pathlib.Path( "package-lock.json" ) ),
)

### -----------------------------------------------------------------------------
import boto3

def cleanup_bucket(bucket_name: str, s3_prefix: str = None, aws_profile_name :str = None) -> None:
    """ Destroys everything UNDER a specific PREFIX of S3 bucket.   """
    # switch to aws_profile named aws_profile_name if it is not None
    if aws_profile_name:
        boto3.setup_default_session(profile_name=aws_profile_name)
    s3 = boto3.client('s3')
    # bucket = s3.Bucket(bucket_name)
    # objects = bucket.objects.filter(Prefix=s3_prefix)
    if s3_prefix is None:
        response = s3.list_objects_v2( Bucket=bucket_name ) ### List all objects in the bucket with the specified s3_prefix
    else:
        response = s3.list_objects_v2( Bucket=bucket_name, Prefix=s3_prefix, ) ### List all objects in the bucket with the specified s3_prefix
    if response:
        objects = response.get('Contents', [])
        if objects:
            for obj in objects:
                s3.delete_object(Bucket=bucket_name, Key=obj['Key'])
                # obj.delete() <-- gets the error --> AttributeError: 'dict' object has no attribute 'delete'
        else:
            print( f"Nothing to delete in bucket {bucket_name} under s3_prefix '{s3_prefix}' in "+ __file__ )
    else:
        print( f"INVALID RESPONSE from list_objects_v2 for bucket {bucket_name} & s3_prefix '{s3_prefix}' in "+ __file__ )

### -----------------------------------------------------------------------------

def checkIfSingleS3ObjectExists(bucket_name: str, s3_prefix: str, aws_profile_name :str = None) -> bool:
    """ Exactly one match to the Prefix is expected.
        Example: if you provide a "folder-s3_prefix" (that has MULTIPLE objects under it), then this function returns False!!!
    """
    if aws_profile_name:
        boto3.setup_default_session(profile_name=aws_profile_name)
    s3 = boto3.client('s3')
    bucket = s3.Bucket(bucket_name)
    objs = list(bucket.objects.filter(Prefix=s3_prefix))
    if DEBUG: print( f"S3-API returned the following - checkIfSingleS3ObjectExists(): "+ __file__ )
    if DEBUG: print( objs )
    return len(objs) == 1 and objs[0].key == s3_prefix
    # return len(objs) > 0 and objs[0].key == s3_prefix

### -----------------------------------------------------------------------------

def checkIfS3PrefixHasObjects(bucket_name: str, s3_prefix: str, aws_profile_name :str = None) -> int:
    """ HOW-MANY object(s) are present under the Prefix?
        returns an INTEGER.  'zero' means no objects found.
    """
    if aws_profile_name:
        boto3.setup_default_session(profile_name=aws_profile_name)
    s3 = boto3.client('s3')
    bucket = s3.Bucket(bucket_name)
    objs = list(bucket.objects.filter(Prefix=s3_prefix))
    return len(objs)
    # return len(objs) > 0 and objs[0].key == s3_prefix

### -----------------------------------------------------------------------------

def downloadFileFromS3(bucketFrom :str, s3_prefix :str, folderPathTo :str, aws_profile_name :str = None) -> pathlib.Path:
    """ Note: returns a Pathlib.Path for downloaded file. _NOT_ the contents of downloaded-file
    """
    if not pathlib.Path(folderPathTo).exists() and not pathlib.Path(folderPathTo).is_dir():
        raise MyException(f"ERROR: folderPathTo '{folderPathTo}' is NOT a directory/folder.")
    ### pathlib.Path.mkdir(folderPathTo, parents=True, exist_ok=True )  <-- in case folderPathTo is a file-path (mistakenly by developer)
    if aws_profile_name:
        boto3.setup_default_session(profile_name=aws_profile_name)
    s3 = boto3.client('s3')
    local_path = folderPathTo +'/'+ pathlib.Path(s3_prefix).name
    s3.download_file( bucketFrom, s3_prefix, local_path )
    if DEBUG: print( f"S3-download Successful to {local_path} at {pathlib.Path.cwd()} - downloadFileFromS3(): "+ __file__ )
    return pathlib.Path(local_path).resolve().absolute()

### -----------------------------------------------------------------------------

def uploadTextToS3(bucketTo: str, s3_prefix: str, text_content: str, aws_profile_name :str = None):
    """ Attention: The 3rd parameter is INLINE textual-content.  __NOT__ the local-filepath!!!
    """
    if aws_profile_name:
        boto3.setup_default_session(profile_name=aws_profile_name)
    s3 = boto3.client('s3')
    s3.Object(bucketTo, s3_prefix).put(Body=text_content)
    if DEBUG: print( "S3-Upload Successful - uploadTextToS3(): "+ __file__ )
    return True

### -----------------------------------------------------------------------------

def copyFolderFromS3(pathFrom, bucketTo, locationTo, aws_profile_name :str = None):
    if aws_profile_name:
        boto3.setup_default_session(profile_name=aws_profile_name)
    s3 = boto3.client('s3')
    response = {}
    response['status'] = 'failed'
    getBucket = pathFrom.split('/')[2]
    location = '/'.join(pathFrom.split('/')[3:])
    if pathFrom.startswith('s3://'):
        copy_source = { 'Bucket': getBucket, 'Key': location }
        uploadKey = locationTo
        recursiveCopyFolderToS3(copy_source,bucketTo,uploadKey, s3client=s3)

### ---------------------------------------------------------------------
def recursiveCopyFolderToS3(src,uplB,uplK, s3client):
    more_objects=True
    found_token = True
    while more_objects:
        if found_token:
            response = s3.list_objects_v2(
                Bucket=src['Bucket'],
                Prefix=src['Key'],
                Delimiter="/")
        else:
            response = s3.list_objects_v2(
                Bucket=src['Bucket'],
                ContinuationToken=found_token,
                Prefix=src['Key'],
                Delimiter="/")
        for source in response["Contents"]:
            raw_name = source["Key"].split("/")[-1]
            raw_name = raw_name
            new_name = os.path.join(uplK,raw_name)
            if raw_name.endswith('_$folder$'):
                src["Key"] = source["Key"].replace('_$folder$','/')
                new_name = new_name.replace('_$folder$','')
                recursiveCopyFolderToS3(src=src, uplB=uplB, uplK=new_name, s3client=s3client)
            else:
                src['Key'] = source["Key"]
                s3.copy_object(CopySource=src,Bucket=uplB,Key=new_name)
                if "NextContinuationToken" in response:
                    found_token = response["NextContinuationToken"]
                    more_objects = True
                else:
                    more_objects = False

### --------------------------------------------------------------------------------------------
## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
### --------------------------------------------------------------------------------------------

if __name__ == "__main__":

    if DEBUG: print( f"len(sys.argv): {len(sys.argv)} inside file: {__file__}" )
    if len(sys.argv) != 4:
        if DEBUG: print( "\n!! ERROR !! missing argument <bucketname> <bucketPREFIX>  <topmost_path>" )
        if DEBUG: print( f"Usage: export PYTHONPATH=$(pwd);     {sys.argv[0]}  <bucketname> <bucketPREFIX>  <topmost_path>" )
        if DEBUG: print( f"Usage: export PYTHONPATH=$(pwd);     {sys.argv[0]}  generic-bucket-591580567012  python-test  UNUSED-PATH " )
        sys.exit(1)

    if DEBUG: print( f"Running script {sys.argv[0]} .. using DEFAULT AWS_PROFILE .." )

    _glue_script_bucketname     = sys.argv[1]
    _glue_script_bucket_prefix  = sys.argv[2]
    topmost_path                = sys.argv[3]
    if DEBUG: print( f"topmost_path (as-is) = '{topmost_path}'" )

    if DEBUG: print( "cwd_contents: " );
    if DEBUG: print( cwd_contents )

    if DEBUG: print( f"\n\n Cleaning out ENTIRE bucket {_glue_script_bucketname} -- EVERYTHING UNDER PREFIX='{_glue_script_bucket_prefix}'" );
    cleanup_bucket(
        bucket_name = _glue_script_bucketname,
        s3_prefix = _glue_script_bucket_prefix,
    )

    if DEBUG: print( f"\n\n Copying LIST-of-FILES into bucket {_glue_script_bucketname}" );
    copy_list_to_bucket(
        local_paths = LIST_of_LOCAL_PATHS,
        bucket_name = _glue_script_bucketname,
        bucket_prefix = _glue_script_bucket_prefix,
    )

    s3_prefix=_glue_script_bucket_prefix +'/'+ 'dynamically-stored-text-content.txt'
    if DEBUG: print( f"\n\n UPLOADING text-string into bucket {_glue_script_bucketname} at PREFIX='{s3_prefix}'" );
    uploadTextToS3(
        text_content="ABC-123 .. " + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),
        bucketTo=_glue_script_bucketname,
        s3_prefix=s3_prefix,
    )

    if DEBUG: print( f"\n\n DOWN-load from bucket {_glue_script_bucketname} from PREFIX='{s3_prefix}'" );
    pth = downloadFileFromS3(
        bucketFrom=_glue_script_bucketname,
        s3_prefix=s3_prefix,
        folderPathTo='/tmp/', ### Warning: Folder-path.  NOT a file-path.
    )
    print( pth )

### EoF
